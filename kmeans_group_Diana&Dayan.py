# -*- coding: utf-8 -*-
"""Copia_de_Kmeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LPDEGRPa0PHajYV4PmxWGua1HqrW1kjW
"""

pip install pyspark

from google.colab import drive
drive.mount('/content/drive')

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
import matplotlib.pyplot as plt

spark = SparkSession.builder.appName("CustomerSegmentation").getOrCreate()

data_path = "/content/drive/MyDrive/Colab Notebooks/Customers_big.csv"

data = spark.read.csv(data_path, header=True, inferSchema=True)
# data.printSchema()
#data.show(5)

from pyspark.sql.functions import when, count, isnull
cols = data.columns
data.select([count(when(isnull(c), c)).alias(c) for c in cols]).show()

from pyspark.sql.functions import isnan

# check schema to find all numeric cols
data.printSchema()

# list all numeric cols
num_cols = ['Age', 'Annual Income ($)', 'Spending Score (1-100)', 'Work Experience', 'Family Size']

# select a count of all the numeric columns where they are NaN and show
data.select([count(when(isnan(c), c)).alias(c) for c in num_cols]).show()

data = data.na.drop()

""" Turn the string-typed 'Gender' Column into a number (0 for female, 1 for male) and add a new column to the dataframe containing numeric gender values"""

indexer = StringIndexer(inputCol="Gender", outputCol="GenderIndex")
data = indexer.fit(data).transform(data)
indexer = StringIndexer(inputCol="Profession", outputCol="ProfessionIndex")
data = indexer.fit(data).transform(data)
#data.show(5)

data.select('Profession','ProfessionIndex').distinct().collect()

# Heatmap of Correlations
from pyspark.ml.stat import Correlation
from pyspark.ml.stat import Correlation
import pandas as pd

# Selecting only numeric columns for correlation calculation
numeric_columns = [col_name for col_name, data_type in data.dtypes if data_type in ['int', 'double']]
numeric_columns.remove('CustomerID')
numeric_df = data.select(numeric_columns)

# Assembling the columns into a single vector
vector_assembler = VectorAssembler(inputCols=numeric_df.columns, outputCol="features")
assembled_df = vector_assembler.transform(numeric_df)

# Calculating the correlation matrix
correlation_matrix = Correlation.corr(assembled_df, "features").head()

# Getting the correlation matrix as a DenseMatrix object
correlation_matrix_values = correlation_matrix[0].toArray()

# Converting the correlation matrix to a Pandas DataFrame
correlation_matrix_pd = pd.DataFrame(correlation_matrix_values, columns=numeric_columns, index=numeric_columns)

# Visualizing the Correlation Matrix using Seaborn and Matplotlib
import seaborn as sns

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(correlation_matrix_pd,
                      annot=True,  # Show the numerical values of correlations
                      cmap="RdBu",
                      vmin=-1,
                      vmax=1)
plt.show()

"""Select the features you want to use, collect them into a single column and add this column to the dataframe"""

input_features = ["Annual Income ($)", "Work Experience","Family Size"]
#input_features = ["Age","Spending Score (1-100)"]
#input_features = ["GenderIndex","Spending Score (1-100)"]
assembler = VectorAssembler(inputCols=input_features,outputCol="features")
data = assembler.transform(data)
data.show(5)

from pyspark.ml.feature import MinMaxScaler
minmscaler = MinMaxScaler(inputCol="features", max=1, min=0, outputCol="featuresNorm")
data = minmscaler.fit(data).transform(data)
data.show()

from pyspark.ml.feature import StandardScaler
standardScaler = StandardScaler(inputCol="features",withMean=True, withStd=True , outputCol="featuresStand")
data = standardScaler.fit(data).transform(data)
# Select "IDCliente" and "scaled_features" and display the results
df_scaled = data.select("CustomerID", "featuresStand")
df_scaled.show()

"""Train the model using the chosen number of clusters, make predictions for the dataset and evaluate using the silhouette score. The silhouette score ranges from -1 to 1 and reflects how close the points in a cluster to each other and how far from the points in other clusters."""

# wcss = Within Cluster Sum of Squares
# Define the range of k values
k_values = list(range(2, 11))
wcss = []

# Create a KMeans object
kmeans = KMeans(featuresCol="featuresStand", predictionCol="prediction", seed=99)

# Iterate over different values of k
for k in k_values:
    kmeans.setK(k)
    model = kmeans.fit(df_scaled)
    cost = model.summary.trainingCost   # Equivalent to wcss.append(kmedias.inertia_) in scikit-learn
    wcss.append(cost)

# Print the results
for k, cost in zip(k_values, wcss):
    print(f"Clusters: {k}, WCSS: {cost}")

import matplotlib.pyplot as plt

# Set the size of the figure
plt.figure(figsize=(6, 4))

# Plotting the WCSS values for different cluster counts
plt.plot(range(2, 11), wcss, marker="o")

# Labeling the axes and title
plt.xlabel("Number of Clusters")
plt.ylabel("Sum of Squared Distances Within the Cluster")
plt.title("Elbow Method")
plt.grid(True)

# Display the plot
plt.show()

n_clusters = []
silhouette_score = []
for n_cluster in range(2, 31):

  kmeans = KMeans(featuresCol="featuresStand", k=n_cluster, maxIter=100, predictionCol='prediction')
  model = kmeans.fit(data)
  predictions = model.transform(data)
  evaluator = ClusteringEvaluator(featuresCol="featuresStand")
  silhouette = evaluator.evaluate(predictions)
  n_clusters.append(n_cluster)
  silhouette_score.append(silhouette)
  print(f"Silhouette Score: {silhouette}", "NCluster:",n_cluster)

#ahora vamos a graficar los resultados
plt.figure(figsize=(10,6))
plt.plot(range(1,30), silhouette_score, color='blue', linestyle='dashed', # x esta en un rango de 1 a 10
         marker='o',markerfacecolor='cyan',markersize=10)
plt.title('silhouette_score vs n_clusters') #agrega el titulo a la grafica
plt.xlabel('n_clusters')
plt.ylabel('silhouette_score')
print('Minumun error: ', min(silhouette_score), 'at n_neighbors', silhouette_score.index(min(silhouette_score))+1) #imprime el menor error y el numero de vecinos para tener este menor error

n_clusters = 8
kmeans = KMeans(featuresCol="featuresStand", k=n_clusters, maxIter=100, predictionCol='prediction',  )
model = kmeans.fit(data)
predictions = model.transform(data)
evaluator = ClusteringEvaluator(featuresCol="featuresStand")
silhouette = evaluator.evaluate(predictions)
print(f"Silhouette Score: {silhouette}", "NCluster:",n_clusters)

cluster_centers = model.clusterCenters()

feature1 = "Annual Income ($)"
feature2 = "Spending Score (1-100)"

plt.figure(figsize=(10,10))
plt.scatter(data.select(feature1).collect(), data.select(feature2).collect(), c=predictions.select("prediction").collect())
plt.scatter([center[input_features.index(feature1)] for center in cluster_centers], [center[input_features.index(feature2)] for center in cluster_centers], marker='x', color='red')
plt.xlabel(feature1)
plt.ylabel(feature2)
plt.title("Customer Segments")
plt.show()

"""Visualize the results"""

cluster_centers = model.clusterCenters()

feature1 = "Annual Income ($)"
feature2 = "Family Size"

plt.figure(figsize=(10,10))
plt.scatter(data.select(feature1).collect(), data.select(feature2).collect(), c=predictions.select("prediction").collect())
plt.scatter([center[input_features.index(feature1)] for center in cluster_centers], [center[input_features.index(feature2)] for center in cluster_centers], marker='x', color='red')
plt.xlabel(feature1)
plt.ylabel(feature2)
plt.title("Customer Segments")
plt.show()

cluster_centers = model.clusterCenters()

feature1 = "Annual Income ($)"
feature2 = "Work Experience"

plt.figure(figsize=(10,10))
plt.scatter(data.select(feature1).collect(), data.select(feature2).collect(), c=predictions.select("prediction").collect())
plt.scatter([center[input_features.index(feature1)] for center in cluster_centers], [center[input_features.index(feature2)] for center in cluster_centers], marker='x', color='red')
plt.xlabel(feature1)
plt.ylabel(feature2)
plt.title("Customer Segments")
plt.show()

dataset= data.toPandas()
dataset.to_numpy()
dataset

####
plt.figure(figsize=(20,8))
from matplotlib import pyplot as plt
dataset.plot(kind='scatter', x='Age', y='Spending Score (1-100)', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

"""Some useful fuctions for the task"""

from matplotlib import pyplot as plt
dataset.plot(kind='scatter', x='Annual Income ($)', y='Family Size', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
dataset.plot(kind='scatter', x='Annual Income ($)', y='Work Experience', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

# Normalization
# from pyspark.ml.feature import MinMaxScaler, StandardScaler
# Removing rows with null values
# data = data.na.drop()

